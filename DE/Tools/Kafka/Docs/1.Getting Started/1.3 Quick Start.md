
- [[#Step 1: Get Kafka|Step 1: Get Kafka]]
- [[#Step 2: Start the Kafka environment|Step 2: Start the Kafka environment]]
	- [[#Step 2: Start the Kafka environment#Using downloaded files|Using downloaded files]]
	- [[#Step 2: Start the Kafka environment#Using JVM Based Apache Kafka Docker Image|Using JVM Based Apache Kafka Docker Image]]
	- [[#Step 2: Start the Kafka environment#Using GraalVM Based Native Apache Kafka Docker Image|Using GraalVM Based Native Apache Kafka Docker Image]]
- [[#Step 3: Create a topic to store your events|Step 3: Create a topic to store your events]]

#### Step 1: Get Kafka

[Download](https://www.apache.org/dyn/closer.cgi?path=/kafka/4.1.1/kafka_2.13-4.1.1.tgz) bản phát hành Kafka mới nhất và giải nén nó:
```bash
$ tar -xzf kafka_2.13-4.1.1.tgz
$ cd kafka_2.13-4.1.1
```

#### Step 2: Start the Kafka environment

NOTE: Your local environment must have Java 17+ installed.

Kafka có thể được chạy bằng cách sử dụng các local scripts và các downloaded files hoặc docker image.
##### Using downloaded files

Tạo Cluster UUID:
```bash
$ KAFKA_CLUSTER_ID="$(bin/kafka-storage.sh random-uuid)"
```

Định dạng Log Directories
```bash
$ bin/kafka-storage.sh format --standalone -t $KAFKA_CLUSTER_ID -c config/server.properties
```

Khởi động Kafka Server
```bash
$ bin/kafka-server-start.sh config/server.properties
```

Sau khi máy chủ Kafka khởi chạy thành công, bạn sẽ có một môi trường Kafka cơ bản đang chạy và sẵn sàng để sử dụng.

##### Using JVM Based Apache Kafka Docker Image

Kéo từ Docker image:
```bash
$ docker pull apache/kafka:4.1.1
```

Khởi động Kafka Docker container:
```bash
$ docker run -p 9092:9092 apache/kafka:4.1.1
```

##### Using GraalVM Based Native Apache Kafka Docker Image

Kéo từ Docker image:
```bash
$ docker pull apache/kafka-native:4.1.1
```

Khởi động Kafka Docker container:
```bash
$ docker run -p 9092:9092 apache/kafka-native:4.1.1
```

#### Step 3: Create a topic to store your events

Kafka là distributed _event streaming platform_ cho phép bạn đọc, ghi, lưu trữ và xử lý [_events_](https://kafka.apache.org/documentation/#messages) (còn gọi là _records_  hoặc  _messages_ trong tài liệu) trên nhiều máy.

Các sự kiện ví dụ bao gồm giao dịch thanh toán, cập nhật vị trí địa lý từ điện thoại di động, đơn đặt hàng vận chuyển, đo lường cảm biến từ thiết bị IoT hoặc thiết bị y tế, v.v. Những sự kiện này được tổ chức và lưu trữ trong các [_topics_](https://kafka.apache.org/documentation/#intro_concepts_and_terms). Nói một cách đơn giản, một chủ đề tương tự như một thư mục trong hệ thống tệp, và các sự kiện là các tệp trong thư mục đó.

Vì vậy, trước khi bạn có thể viết sự kiện đầu tiên, bạn phải tạo một chủ đề. Mở một phiên terminal khác và chạy:
```bash
$ bin/kafka-topics.sh --create --topic quickstart-events --bootstrap-server localhost:9092
```

Tất cả các công cụ dòng lệnh của Kafka đều có các tùy chọn bổ sung: chạy lệnh `kafka-topics.sh` mà không có bất kỳ đối số nào để hiển thị thông tin sử dụng. Ví dụ: nó cũng có thể hiển thị cho bạn các chi tiết như số lượng phân vùng ([details such as the partition count](https://kafka.apache.org/documentation/#intro_concepts_and_terms)) của chủ đề mới:
```bash
$ bin/kafka-topics.sh --describe --topic quickstart-events --bootstrap-server localhost:9092
Topic: quickstart-events        TopicId: NPmZHyhbR9y00wMglMH2sg PartitionCount: 1       ReplicationFactor: 1	Configs:
Topic: quickstart-events Partition: 0    Leader: 0   Replicas: 0 Isr: 0
```

#### Step 4: Write some events into the topic

Kafka client giao tiếp với các Kafka broker qua network để ghi (hoặc đọc) sự kiện. Sau khi nhận được, các broker sẽ lưu trữ sự kiện theo cách bền vững và có khả năng chịu lỗi trong thời gian dài tùy theo nhu cầu của bạn, thậm chí là mãi mãi.

Chạy trình khách console producer để ghi một vài sự kiện vào chủ đề của bạn. Theo mặc định, mỗi dòng bạn nhập sẽ tạo ra một sự kiện riêng biệt được ghi vào chủ đề.
```bash
$ bin/kafka-console-producer.sh --topic quickstart-events --bootstrap-server localhost:9092
>This is my first event
>This is my second event
```

Bạn có thể dừng chương trình producer client bằng phím `Ctrl-C` bất cứ lúc nào.

#### Step 5: Read the events

Mở một phiên đầu cuối khác và chạy trình khách hàng bảng điều khiển để đọc các sự kiện bạn vừa tạo:
```bash
$ bin/kafka-console-consumer.sh --topic quickstart-events --from-beginning --bootstrap-server localhost:9092
This is my first event
This is my second event
```

Bạn có thể dừng chương trình consumer client bằng phím `Ctrl-C` bất cứ lúc nào.

Bạn có thể thoải mái thử nghiệm: ví dụ, hãy quay lại terminal của producer (bước trước) để ghi thêm các sự kiện mới, và bạn sẽ thấy các sự kiện đó xuất hiện ngay lập tức trong terminal của consumer.

Vì các sự kiện được lưu trữ bền vững trong Kafka, bạn có thể đọc chúng nhiều lần và từ nhiều consumer tùy ý. Bạn có thể dễ dàng kiểm chứng điều này bằng cách mở thêm một phiên terminal khác và chạy lại lệnh trước đó.

#### Step 6: Import/export your data as streams of events with Kafka Connect

Bạn có thể có rất nhiều dữ liệu trong các hệ thống hiện có như cơ sở dữ liệu quan hệ hoặc hệ thống nhắn tin truyền thống, cùng với nhiều ứng dụng đã sử dụng các hệ thống này.[Kafka Connect](https://kafka.apache.org/documentation/#connect) cho phép bạn liên tục nhập dữ liệu(ingest data) từ các hệ thống bên ngoài vào Kafka và ngược lại. Đây là một công cụ mở rộng chạy các trình kết nối, triển khai logic tùy chỉnh để tương tác với hệ thống bên ngoài. Do đó, việc tích hợp các hệ thống hiện có với Kafka rất dễ dàng. Để quá trình này trở nên dễ dàng hơn nữa, có hàng trăm trình kết nối như vậy sẵn có.

Trong phần hướng dẫn nhanh này, chúng ta sẽ xem cách chạy Kafka Connect bằng các trình kết nối đơn giản để nhập dữ liệu từ tệp vào chủ đề Kafka và xuất dữ liệu từ chủ đề Kafka vào tệp.

Trước tiên, hãy đảm bảo thêm `connect-file-4.1.1.jar` vào thuộc tính `plugin.path` trong cấu hình của Connect worker. Trong hướng dẫn quickstart này, chúng ta sẽ sử dụng đường dẫn tương đối và coi gói kết nối như một uber jar, hoạt động khi các lệnh khởi động nhanh được chạy từ thư mục cài đặt. Tuy nhiên, cần lưu ý rằng đối với các triển khai sản xuất, sử dụng đường dẫn tuyệt đối luôn là lựa chọn tốt hơn. Xem [plugin.path](https://kafka.apache.org/documentation/#connectconfigs_plugin.path) để biết mô tả chi tiết về cách thiết lập cấu hình này.

Chỉnh sửa tệp `config/connect-standalone.properties` thêm hoặc thay đổi thuộc tính cấu hình `plugin.path` cho phù hợp với nội dung sau và lưu tệp:
```bash
$ echo "plugin.path=libs/connect-file-4.1.1.jar" >> config/connect-standalone.properties
```

Sau đó, bắt đầu bằng cách tạo một số seed data để thử nghiệm:
```bash
$ echo -e "foo\nbar" > test.txt
```

Hoặc trên windows:
```bash
$ echo foo > test.txt
$ echo bar >> test.txt
```

Tiếp theo, chúng ta sẽ khởi động hai trình kết nối chạy ở chế độ _standalone_, nghĩa là chúng chạy trong một quy trình cục bộ, chuyên dụng duy nhất. Chúng tôi cung cấp ba tệp cấu hình làm tham số. Tệp đầu tiên luôn là cấu hình cho quy trình Kafka Connect, chứa các cấu hình chung như các trình Kafka brokers để kết nối và định dạng tuần tự hóa cho dữ liệu. Các tệp cấu hình còn lại, mỗi tệp chỉ định một trình kết nối cần tạo. Các tệp này bao gồm một tên trình kết nối duy nhất, lớp trình kết nối cần khởi tạo và bất kỳ cấu hình nào khác mà trình kết nối yêu cầu.
```bash
$ bin/connect-standalone.sh config/connect-standalone.properties config/connect-file-source.properties config/connect-file-sink.properties
```

Các tệp cấu hình mẫu này, có trong Kafka, sử dụng cấu hình cụm cục bộ mặc định mà bạn đã bắt đầu trước đó và tạo hai trình kết nối: trình kết nối đầu tiên là source connector đọc các dòng từ tệp đầu vào và tạo từng dòng thành một chủ đề Kafka và trình kết nối thứ hai là sink connector đọc các thông báo từ chủ đề Kafka và tạo từng dòng thành một dòng trong tệp đầu ra.

Trong quá trình khởi động, bạn sẽ thấy một số thông báo nhật ký, bao gồm một số thông báo cho biết các trình kết nối đang được khởi tạo. Khi quá trình Kafka Connect bắt đầu, source connector sẽ bắt đầu đọc các dòng từ `test.txt` và tạo chúng cho chủ đề connect-test, và sink connector sẽ bắt đầu đọc các thông báo từ chủ đề connect-test và ghi chúng vào tệp `test.sink.txt`. Chúng ta có thể xác minh dữ liệu đã được phân phối qua toàn bộ đường ống bằng cách kiểm tra nội dung của tệp đầu ra:
```bash
$ more test.sink.txt
foo
bar
```

Lưu ý rằng dữ liệu đang được lưu trữ trong chủ đề Kafka connect-test, do đó chúng ta cũng có thể chạy trình tiêu thụ bảng điều khiển để xem dữ liệu trong chủ đề (hoặc sử dụng mã người tiêu thụ tùy chỉnh để xử lý dữ liệu):